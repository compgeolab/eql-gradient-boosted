{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridding a synthetic ground survey\n",
    "\n",
    "Let's perform a synthetic ground survey of the synthetic model made out of prisms and try to interpolate the observed data on a regular grid.\n",
    "Because the model is synthetic, we can compute the true value of the field on this regular grid. Therefore, we have an objective way to score the interpolation. This allow us to objectively compare the different source layouts.\n",
    "\n",
    "Firstly, we want to import useful packages and define some minor functions that will help us to visualize better the notebook cells and reduce ammount of code on them.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "- [1. Define parameters used on the gridding](#1.-Define-parameters-used-on-the-gridding)\n",
    "- [2. Create the synthetic ground survey](#2.-Create-the-synthetic-ground-survey)\n",
    "- [3. Compute the field of the synthetic model on a grid](#3.-Compute-the-field-of-the-synthetic-model-on-a-grid)\n",
    "- [4. Grid data with constant depth](#4.-Grid-data-with-constant-depth)\n",
    "    - [4.1. One source beneath each data point](#4.1.-One-source-beneath-each-data-point)\n",
    "    - [4.2. Source beneath block reduced observation points](#4.2.-Source-beneath-block-reduced-observation-points)\n",
    "    - [4.3. Regular grid of source points](#4.3.-Regular-grid-of-source-points)\n",
    "- [5. Grid data with constant depth](#5.-Grid-data-with-relative-depth)\n",
    "    - [5.1. One source beneath each data point](#5.1.-One-source-beneath-each-data-point)\n",
    "    - [5.2. Source beneath block reduced observation points](#5.2.-Source-beneath-block-reduced-observation-points)\n",
    "- [6. Grid data with variable relative depth](#6.-Grid-data-with-variable-relative-depth)\n",
    "    - [6.1. One source beneath each data point](#6.1.-One-source-beneath-each-data-point)\n",
    "    - [6.2. Source beneath block reduced observation points](#6.2.-Source-beneath-block-reduced-observation-points)\n",
    "- [7. Plot and compare the best predictions](#7.-Plot-and-compare-the-best-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import useful packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import itertools\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import verde as vd\n",
    "import harmonica as hm\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from eql_source_layouts import (\n",
    "    synthetic_model,\n",
    "    source_bellow_data,\n",
    "    block_reduced_sources,\n",
    "    grid_sources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions to plot gridding results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(\n",
    "    grid,\n",
    "    prediction,\n",
    "    target,\n",
    "    title=\"Prediction\",\n",
    "    figsize=(12, 6),\n",
    "    shrink_cbar=0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot prediction and comparison with target\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=figsize)\n",
    "    tmp = ax1.pcolormesh(*grid[:2], prediction)\n",
    "    plt.colorbar(tmp, ax=ax1, shrink=shrink_cbar, label=field_units)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    maxabs = vd.maxabs(target - prediction)\n",
    "    tmp = ax2.pcolormesh(\n",
    "        *grid[:2],\n",
    "        target - best_prediction,\n",
    "        vmin=-maxabs,\n",
    "        vmax=maxabs,\n",
    "        cmap=\"seismic\",\n",
    "    )\n",
    "    plt.colorbar(tmp, ax=ax2, shrink=shrink_cbar, label=field_units)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(\"Difference between target and prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def histogram_errors(prediction, target, bins=20):\n",
    "    \"\"\"\n",
    "    Plot histogram of prediction errors\n",
    "    \"\"\"\n",
    "    plt.hist((target - prediction).ravel(), bins=bins)\n",
    "    plt.title(\"Histogram of prediction errors\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function to print prediction parameters and scores with style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params_scores(parameters_sets, scores):\n",
    "    \"\"\"\n",
    "    Display parameters and scores with style\n",
    "    \n",
    "    It also highlights the row that contains the maximum score\n",
    "    \"\"\"\n",
    "    df = {}\n",
    "    for keys in parameters_sets[0]:\n",
    "        df[keys] = []\n",
    "    df[\"score\"] = []\n",
    "    for parameters, score in zip(parameters_sets, scores):\n",
    "        for key, param in parameters.items():\n",
    "            df[key].append(param)\n",
    "        df[\"score\"].append(score)\n",
    "    df = pd.DataFrame(df)\n",
    "    display(df.style.apply(highlight_max_row, column=\"score\", axis=None))\n",
    "    return df\n",
    "\n",
    "\n",
    "def highlight_max_row(df, column, color=\"orange\"):\n",
    "    \"\"\"\n",
    "    Highlight the row that contains the max value of a column\n",
    "    \"\"\"\n",
    "    style = df.copy()\n",
    "    is_max = (df[column] == df[column].max())\n",
    "    background = [\n",
    "        'background-color: {}'.format(color) if i else '' for i in is_max\n",
    "    ]\n",
    "    for col in style:\n",
    "        style[col] = background\n",
    "    return style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define parameters used on the gridding\n",
    "\n",
    "Let's define all the parameters that will be used on this notebook in the following cells. These control the results that will be obtain on the rest of the notebook. If you want to change something (like the computation height of the grid, the survey region, interpolation parameters, etc), you can just do it here.\n",
    "\n",
    "We will avoid hardcoding parameters outside of these few cells in order to facilitate reproducibility and keep things more organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a survey region of 1 x 1 degrees (~ 100km x 100km)\n",
    "region_degrees = (-0.5, 0.5, -0.5, 0.5)\n",
    "\n",
    "# Define bottom and top of the synthetic model\n",
    "model_bottom, model_top = -10e3, 0\n",
    "\n",
    "# Define which field will be meassured\n",
    "field = \"g_z\"\n",
    "field_units = \"mGal\"\n",
    "\n",
    "# Define standard deviation for the Gaussian noise that\n",
    "# will be added to the synthetic survey (in mGal)\n",
    "noise_std = 1\n",
    "\n",
    "# Define a seed to reproduce the same results on each run \n",
    "np.random.seed(12345)\n",
    "\n",
    "\n",
    "# Define the spacing of the target regular grid\n",
    "# and its observation height\n",
    "grid_spacing = 2e3\n",
    "grid_height = 2000\n",
    "\n",
    "# Define set of interpolation parameters\n",
    "# ======================================\n",
    "# Define dampings used on every fitting of the gridder\n",
    "dampings = [None, 1e-4, 1e-3, 1e-2]\n",
    "# Define values of constant depth\n",
    "constant_depths = [1e3, 2e3, 5e3, 10e3, 15e3]\n",
    "# Define values of relative depth\n",
    "relative_depths = [1e3, 2e3, 5e3, 10e3, 15e3]\n",
    "# Define parameters for the grid layout:\n",
    "#    spacing, depth and padding\n",
    "source_grid_spacings = [0.5e3, 1e3, 2e3]\n",
    "source_grid_depths = [1e3, 2e3, 5e3]\n",
    "source_grid_paddings = [0, 0.1, 0.2]\n",
    "# Define parameters for variable relative depth layouts:\n",
    "#    depth factor, depth shift and k_values\n",
    "depth_factors = [0.5, 1, 5, 10]\n",
    "depth_shifts = [-100, -1000, -5000]\n",
    "k_values = [1, 10]\n",
    "# We will set the block spacing for the block reduced\n",
    "# layouts equal to the target grid spacing\n",
    "block_spacing = grid_spacing\n",
    "\n",
    "# Finally, define a dict where the best predictions will be\n",
    "# stored to plot them altogether at the end of the notebook.\n",
    "best_predictions = {\n",
    "    \"constant_depth\": {},\n",
    "    \"relative_depth\": {},\n",
    "    \"variable_relative_depth\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the synthetic ground survey\n",
    "\n",
    "To create the ground survey we need to load the synthetic model made out of prisms, the location of the observation points and then compute the field that the prisms generate on those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates of observation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = hm.synthetic.ground_survey(region=region_degrees)\n",
    "display(survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project survey points into Cartesian coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = pyproj.Proj(proj=\"merc\", lat_ts=0)\n",
    "survey[\"easting\"], survey[\"northing\"] = projection(\n",
    "    survey.longitude.values, survey.latitude.values\n",
    ")\n",
    "display(survey)\n",
    "\n",
    "# Define region boundaries in projected coordinates\n",
    "region = (\n",
    "    survey.easting.values.min(),\n",
    "    survey.easting.values.max(),\n",
    "    survey.northing.min(),\n",
    "    survey.northing.max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the survey points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tmp = ax.scatter(survey.easting, survey.northing, c=survey.height, s=6)\n",
    "plt.colorbar(tmp, ax=ax, label=\"m\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Height of ground survey points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the synthetic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model region\n",
    "model_region = tuple(list(region) + [model_bottom, model_top])\n",
    "\n",
    "# Create synthetic model\n",
    "model = synthetic_model(model_region)\n",
    "print(model.keys())\n",
    "print(\"Number of prisms: {}\".format(len(model[\"densities\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.add_collection(PatchCollection(model[\"rectangles\"], match_original=True))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Synthetic model made out of prisms\")\n",
    "ax.set_xlim(region[:2])\n",
    "ax.set_ylim(region[2:4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gravity field (g_z) on the observation points in mGal and add Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = (survey.easting, survey.northing, survey.height)\n",
    "survey[field] = hm.prism_gravity(\n",
    "    coordinates, model[\"prisms\"], model[\"densities\"], field=field\n",
    ") + np.random.normal(scale=noise_std, size=survey.easting.size)\n",
    "display(survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the observed field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tmp = ax.scatter(survey.easting, survey.northing, c=getattr(survey, field), s=6)\n",
    "plt.colorbar(tmp, ax=ax, label=field_units)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Synthetic ground survey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the field of the synthetic model on a grid\n",
    "\n",
    "Now, let's compute the gravity field that the synthetic model generates on the regular grid. These results will serve as a target for the interpolations using different source layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the regular grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = vd.grid_coordinates(\n",
    "    region=region, spacing=grid_spacing, extra_coords=grid_height\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the synthetic gravity field on the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = hm.prism_gravity(\n",
    "    grid, model[\"prisms\"], model[\"densities\"], field=field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot target gravity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tmp = ax.pcolormesh(*grid[:2], target)\n",
    "plt.colorbar(tmp, ax=ax, shrink=0.7, label=field_units)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Target grid values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid data with constant depth\n",
    "\n",
    "Now, let's grid the synthetic data through different source layouts, but keeping the constant depth approach.\n",
    "We will compare each interpolation with the target data in order to assess for the quality of each source layout.\n",
    "\n",
    "Because the gridder could take several parameters that control the interpolation, we will perform several predictions with different set of parameters and then score each one of them against the target data. We will choose the best prediction as the one that produces the highest score. Because this score is computed agains *true data*, it's an objective quality estimator of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. One source beneath each data point\n",
    "\n",
    "Let's perform several interpolations on the grid by setting a single source point beneath each data point at a constant depth. This can be achieved throught `harmonica.EQLHarmonic` by passing a points layout build through the `source_bellow_data()` function.\n",
    "Each interpolation is performed with different values of the `damping` and `constant_depth` parameters. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(damping=combo[0], constant_depth=combo[1])\n",
    "    for combo in itertools.product(dampings, constant_depths)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the source points layout by putting one source beneath\n",
    "    # each data point\n",
    "    points = source_bellow_data(\n",
    "        coordinates, depth_type=\"constant_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = source_bellow_data(\n",
    "    coordinates, depth_type=\"constant_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"constant_depth\"][\"source_beneath_data\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Source beneath block reduced observation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same, but with a different source layout: block reduce the observation points and put one source beneath each reduced coordinate at a constant depth.\n",
    "For that we will need to make use of our `block_reduced_sources()` function.\n",
    "Each interpolation will be performed with different values of the `damping` and `constant_depth` parameters. The `spacing` parameter will be set equal to the target grid spacing. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        constant_depth=combo[1],\n",
    "        spacing=block_spacing,\n",
    "    )\n",
    "    for combo in itertools.product(dampings, constant_depths)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the custom source points layout by block reducing the\n",
    "    # observed coordinates and setting the point sources at a \n",
    "    # constant depth beneath them.\n",
    "    points = block_reduced_sources(\n",
    "        coordinates, depth_type=\"constant_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points layout and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = block_reduced_sources(\n",
    "    coordinates, depth_type=\"constant_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"constant_depth\"][\"block_reduced\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Regular grid of source points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a regular grid of source points located at a constant depth beneath the mean height of observation points.\n",
    "Each interpolation will be performed with different values of the `damping` and `constant_depth`. The `spacing` parameter will be set equal to the target grid spacing. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        constant_depth=combo[1],\n",
    "        pad=combo[2],\n",
    "        spacing=combo[3],\n",
    "    )\n",
    "    for combo in itertools.product(\n",
    "        dampings,\n",
    "        source_grid_depths,\n",
    "        source_grid_paddings,\n",
    "        source_grid_spacings,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data. When fitting the gridder, it will raise a warning due to the fact that the determination of the coefficients for each source point is an under-determined problem: the number of point sources is bigger than the number of observation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean height of observation points\n",
    "mean_height = survey.height.mean()\n",
    "\n",
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the custom source points layout by putting them on a\n",
    "    # regular grid at a constant depth beneath the data points.\n",
    "    points = grid_sources(\n",
    "        coordinates, depth_type=\"constant_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points layout and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = grid_sources(\n",
    "    coordinates, depth_type=\"constant_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"constant_depth\"][\"grid\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grid data with relative depth\n",
    "\n",
    "Now, let's grid the synthetic data through different source layouts, but keeping the relative depth approach.\n",
    "We will compare each interpolation with the target data in order to assess for the quality of each source layout.\n",
    "\n",
    "Because the gridder could take several parameters that control the interpolation, we will perform several predictions with different set of parameters and then score each one of them against the target data. We will choose the best prediction as the one that produces the highest score. Because this score is computed agains *true data*, it's an objective quality estimator of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. One source beneath each data point\n",
    "\n",
    "Let's perform several interpolations on the grid by setting a single source point beneath each data point at a relative depth. This can be achieved throught `harmonica.EQLHarmonic` by passing a points layout build through the `source_bellow_data()` function.\n",
    "Each interpolation is performed with different values of the `damping` and `relative_depth` parameters. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        relative_depth=combo[1],\n",
    "    )\n",
    "    for combo in itertools.product(dampings, relative_depths)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the source points layout by putting one source beneath\n",
    "    # each data point\n",
    "    points = source_bellow_data(\n",
    "        coordinates, depth_type=\"relative_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = source_bellow_data(\n",
    "    coordinates, depth_type=\"relative_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"relative_depth\"][\"source_beneath_data\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Source beneath block reduced observation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same, but with a different source layout: block reduce the observation points and put one source beneath each reduced coordinate at a relative depth.\n",
    "For that we will need to make use of our `block_reduced_sources()` function.\n",
    "Each interpolation will be performed with different values of the `damping` and `relative_depth` parameters. The `spacing` parameter will be set equal to the target grid spacing. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        relative_depth=combo[1],\n",
    "        spacing=block_spacing,\n",
    "    )\n",
    "    for combo in itertools.product(dampings, relative_depths)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the custom source points layout by block reducing the\n",
    "    # observed coordinates and setting the point sources at a\n",
    "    # relative depth beneath them.\n",
    "    points = block_reduced_sources(\n",
    "        coordinates, depth_type=\"relative_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points layout and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = block_reduced_sources(\n",
    "    coordinates, depth_type=\"relative_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"relative_depth\"][\"block_reduced\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grid data with variable relative depth\n",
    "\n",
    "We will perform the same steps as before, but now with variable relative depth.\n",
    "For every source layout we will use our `variable_relative_depth()` function to compute the depth of each source point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. One source beneath each data point\n",
    "\n",
    "Let's perform several interpolations on the grid by setting a single source point beneath each data point at a variable relative depth.\n",
    "Each interpolation will be performed with different values of the `damping`, `depth_factor`, `depth_shift` and`k_nearest` parameters. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        depth_factor=combo[1],\n",
    "        depth_shift=combo[2],\n",
    "        k_nearest=combo[3],\n",
    "    )\n",
    "    for combo in itertools.product(\n",
    "        dampings, depth_factors, depth_shifts, k_values\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the source points layout by putting one source beneath\n",
    "    # each data point.\n",
    "    points = source_bellow_data(\n",
    "        coordinates, depth_type=\"variable_relative_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = source_bellow_data(\n",
    "    coordinates, depth_type=\"variable_relative_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"variable_relative_depth\"][\"source_beneath_data\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Source beneath block reduced observation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same, but with a different source layout: block reduce the observation points and put one source beneath each reduced coordinate at a variable relative depth.\n",
    "For that we will need to make use of our `block_reduce_points()` function.\n",
    "Each interpolation will be performed with different values of the `damping`, `depth_factor`, `depth_shift` and `k_nearest` parameters. The `spacing` parameter will be set equal to the target grid spacing. Then we score the prediction against the target field and store all these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of parameters that will be used on the interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sets = [\n",
    "    dict(\n",
    "        damping=combo[0],\n",
    "        spacing=block_spacing,\n",
    "        depth_factor=combo[1],\n",
    "        depth_shift=combo[2],\n",
    "        k_nearest=combo[3],\n",
    "    )\n",
    "    for combo in itertools.product(\n",
    "        dampings, depth_factors, depth_shifts, k_values\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the interpolations and score them against the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for parameters in parameters_sets:\n",
    "    # Create the custom source points layout by block reducing the\n",
    "    # observed coordinates and setting the point sources at a\n",
    "    # relative depth beneath them.\n",
    "    points = block_reduced_sources(\n",
    "        coordinates, depth_type=\"variable_relative_depth\", **parameters\n",
    "    )\n",
    "    # Initialize the gridder passing the points layout and the damping\n",
    "    eql = hm.EQLHarmonic(points=points, damping=parameters[\"damping\"])\n",
    "    # Fit the gridder giving the survey data\n",
    "    eql.fit(coordinates, getattr(survey, field))\n",
    "    # Predict the field on the regular grid\n",
    "    prediction = eql.predict(grid)\n",
    "    # Score the prediction against target data\n",
    "    scores.append(r2_score(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all the computed scores. The highlighted row indicates the set of parameters that produces the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_params_scores(parameters_sets, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the best prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax(scores)\n",
    "best_params = parameters_sets[best]\n",
    "print(\"Best score:\", scores[best])\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "points = block_reduced_sources(\n",
    "    coordinates, depth_type=\"variable_relative_depth\", **best_params\n",
    ")\n",
    "eql = hm.EQLHarmonic(points=points, damping=best_params[\"damping\"])\n",
    "eql.fit(coordinates, getattr(survey, field))\n",
    "best_prediction = eql.predict(grid)\n",
    "\n",
    "print(\"Number of point sources:\", points[0].size)\n",
    "\n",
    "# Store the best prediction\n",
    "best_predictions[\"variable_relative_depth\"][\"block_reduced\"] = best_prediction\n",
    "\n",
    "plot_prediction(grid, best_prediction, target)\n",
    "histogram_errors(best_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot and compare the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max absolute value of the difference between target an all predictions\n",
    "# in order to plot every difference with the same color scale.\n",
    "maxabs = vd.maxabs(\n",
    "    tuple(\n",
    "        target - i\n",
    "        for subset in best_predictions.values()\n",
    "        for i in subset.values()\n",
    "    )\n",
    ")\n",
    "maxabs = min(maxabs, 6)\n",
    "\n",
    "# Initialize figure\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=3,\n",
    "    figsize=(15, 15),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "# Plot the differences between the target and the best prediction for each layout\n",
    "axes = axes.T\n",
    "for ax_col, depth_type in zip(axes, best_predictions):\n",
    "    for ax, layout in zip(ax_col, best_predictions[depth_type]):\n",
    "        prediction = best_predictions[depth_type][layout]\n",
    "        tmp = ax.pcolormesh(\n",
    "            *grid[:2],\n",
    "            target - prediction,\n",
    "            vmin=-maxabs,\n",
    "            vmax=maxabs,\n",
    "            cmap=\"seismic\",\n",
    "        )\n",
    "        ax.scatter(survey.easting, survey.northing, s=1, alpha=0.2, color=\"k\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_title(\"{} (r2: {:.3f})\".format(layout, r2_score(target, prediction)))      \n",
    "\n",
    "# Hide the last two axes because they are not used\n",
    "axes[-1][-1].set_visible(False)\n",
    "axes[-2][-1].set_visible(False)\n",
    "\n",
    "# Annotate the columns of the figure\n",
    "for i, depth_type in enumerate(list(best_predictions.keys())):\n",
    "    axes[i][0].text(\n",
    "        .5,\n",
    "        1.1,\n",
    "        depth_type,\n",
    "        fontsize=\"large\",\n",
    "        fontweight=\"bold\",\n",
    "        horizontalalignment='center',\n",
    "        transform=axes[i][0].transAxes,\n",
    "    )\n",
    "\n",
    "# Add colorbar\n",
    "fig.subplots_adjust(bottom=0.1, wspace=0.05)\n",
    "cbar_ax = fig.add_axes([0.15, 0.05, 0.7, 0.02])\n",
    "fig.colorbar(tmp, cax=cbar_ax, orientation=\"horizontal\", label=field_units)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
