\documentclass[twocolumn]{article}
\usepackage[left=0.7in,right=0.7in,top=1in,bottom=1in]{geometry}
\setlength{\columnsep}{2\columnsep}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{fancyhdr}


% Define title, authors, affiliations and DOI
% ===========================================
\newcommand{\Title}{
    A better strategy for interpolating gravity and magnetic data
}
\newcommand{\Author}{
    S.R. Soler,
    L. Uieda
}
\newcommand{\AuthorAffil}{
    {\large
        S.R. Soler$^{1,2}$,
        L. Uieda$^{3}$
    }
    \\[0.4cm]
    {\small $^{1}$CONICET, Argentina (santiago.r.soler@gmail.com)} \\
    {\small $^{2}$Instituto Geofísico Sismológico Volponi, UNSJ, Argentina} \\
    {\small $^{3}$University of Liverpool} \\

}
\newcommand{\DOI}{
    doi:\href{https://doi.org/xxx.xxx/xxxxxx}{10.1093/xxx.xxx/xxxxxx}
}
\newcommand{\DOILink}{
    \href{https://doi.org/xxx.xxx/xxxxxx}{doi.org/xxx.xxx/xxxxxx}
}


% Configure header and hypersetup
% ===============================
\pagestyle{fancy}
\fancyhf{}
\lhead{
    \fontsize{9pt}{12pt}\selectfont
    \Author{}, 2019. \DOI{}
}
\rhead{\fontsize{9pt}{12pt}\selectfont \thepage}
\renewcommand{\headrulewidth}{0pt}
\hypersetup{
    allcolors=blue,
    pdftitle={\Title},
    pdfauthor={\Author},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{\Title}
\author{\AuthorAffil}
\date{
    \normalsize
    \today
}
\maketitle

\begin{abstract}
    My abstract
    \\[0.5cm]
    \textbf{Keywords:}
    My keywords
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Potential field data, like gravity and magnetics, are useful for geophysics
exploration.
The potential field data can be obtained from ground, airborne or satellite
measurements.
On ground surveys the data is often gathered on scattered points or following
irregular paths, usually at the topographic surface.
The data points have different heights.
On airborne surveys the data is gathered on flight paths, producing a large
number of data points concentrated along almost straight lines.
The height of measurement could change because of the vertical movement of the
airship.
To improve visualization of the data for interpretation and to prepare the data
for processing through different methods (RTP, upward continuation, Euler
deconvolution, etc), the data needs to be interpolated on a regular grid of
data points.

There are several methods for interpolating 2D data like minimum curvature,
biharmonic Splines or Kriging.
These all-purpose methods show a few problems when applied to potential field
data.
(i) They don't take into account the height of the observation points.
Potential field data has a strong dependency on measurement height.
(ii) The predicted grid data is not necessary an Harmonic function.
(iii) They don't usually take into account the anisotropy of the observation
points in ground and airborne surveys.

One of the most widely used methods for interpolating gravity and magnetic data
is the equivalent sources technique (also known as equivalent layer, radial
basis functions or Green's functions interpolation).
It consists in estimating a source distribution that produces the same field as
the one measured and using this estimate to predict new values.
\citet{dampney1969} was the first to introduce the equivalent source technique.

The EQL can be used for interpolation of potential field data
\citep{cordell1992, cooper2000}.
It has been also used for applying a reduction to the pole to magnetic data
\citep{guspi2009, silva1986, emilia1973, nakatsuka2006} or an upward
continuation (cite).

The most widely used source distribution is a set of point sources.
The potential field of point sources is easy to compute.
\citet{barnes2011} make use of prisms.
How to locate these point masses?
Existing approaches:
(i) one point source beneath each observation point at a constant depth (cite),
(ii) one point source beneath each observation point at a depth proportional to
the distance to its nearest data points (cite),
(iii) a regular grid of sources \citep{barnes2011}.

We want to find out which distribution of sources produces the best
interpolations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The equivalent sources technique}

Mathematical background of the EQL\@.
How it can be understood in terms of Green's functions problem.
EQL implementation on Harmonica.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Source distributions}

% Describe the proposed source distributions as combination of layouts and
% depths strategies.

The choice of a source distribution is not trivial and plays an important role
on the accuracy of the predictions.

The most widely used source distributions are: a regular grid of point sources
and one point source beneath each observation point.

The \emph{regular grid} creates an homogeneous distribution of point sources
bellow the surveyed region.
It could create too many sources on areas without data and very few ones on
areas with a high number of observation points, leading to an underfitting of
the observed data.
This could be solved by reducing the grid spacing and therefore increasing the
number of point sources and the computational load of the interpolation
process.

The \emph{source beneath data} layout adds sources where they are more needed.
If the survey is composed by a large number of data points clustered on paths
(like an airborne survey), putting one source beneath each observation point
will create an anisotropic source distribution: there will be several sources
distributed along a privileged direction.
This could introduce aliases on the predictions and increase the computational
load for fitting the predictor.

We propose a new source distribution that could simultaneously solve the
problems of the preceding ones: the \emph{block-median sources}.
It consist in dividing the region in blocks of equal size, computing the median
location of the observation points that fall inside each block, and putting one
point source bellow this block-median coordinate.
It creates one point source beneath each populated block, solving the problem
of the \emph{grid sources} layout, but also reducing the number of point
sources in comparison with the \emph{source beneath data} layout.

The depth of the point sources can be chosen following different criteria.
Deep sources generate low frequencies, while shallow ones generate high
frequencies (cite).
The most simple option is to locate all point sources at the same depth, which
we will call \emph{constant depth} in the future.
If the measurement where taken at significantly different altitudes, the
elevated computation points will be more distant to the sources than the lower
ones.
This may create problems on reproducing high frequencies on the elevated
points.

One possible solution is to locate each source bellow its corresponding
observation point or block-median point at a constant \emph{relative depth}
from it.
The sources won't be all at the same depth, but they will all be at the same
distance from their corresponding observation point or block-median point.
It worth mention that this approach could not be applied to \emph{grid
sources}.

In case our survey presents heavily clustered data points, we may want to
reduce the depth of the sources bellow that region in order to reproduce the
high frequencies measured by these clustered observation points.
We can propose a \emph{variable relative depth} approach that locates each
point source to a depth proportional to the mean distance to its $k$ nearest
source neighbours \citep{cordell1992, guspi2004, guspi2009}.
We propose to add a static shift to the computed depth in order to prevent very
shallow sources in case the mean distance to the $k$ nearest neighbours is too
small.
In summary, the depth of the sources can be set as follows:

\begin{equation}
    \textrm{depth} =
        \textrm{depth_factor} \textrm{mean_distance} + \textrm{static_shift}
\end{equation}

\noindent where the $\textrm{depth_factor}$ and the $\textrm{static_shift}$ are
parameters and the $\textrm{mean_distance}$ is the mean distance to the $k$
nearest source neighbours.


The combination of the three source layouts with the three strategies to define
the depth of the sources define seven different source distributions (see
Table~\ref{tab:source-distributions}).
The \emph{grid sources} is only compatible with the \emph{constant depth}
scheme.


\begin{table*}
\begin{minipage}{80mm}
    \caption{
        Source distributions as combinations of source layouts and depth
        strategies.
    }
    \label{tab:source-distributions}
    \begin{tabular}{lccc}
        & Source bellow data & Block-median sources & Grid sources \\ \hline
        Constant depth          & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
        Relative depth          & $\checkmark$ & $\checkmark$ & $\times$     \\
        Variable relative depth & $\checkmark$ & $\checkmark$ & $\times$     \\
    \end{tabular}
\end{minipage}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparison of source distributions}

Compare the accuracy of the predictions made by each source distribution.
Need an objective way to compare them.
Propose a synthetic gravity model made out of prisms of different sizes and
density contrasts.
Create a regular grid of observation points where the gravitational effect of
the synthetic model will be computed.
Call it the \emph{target grid}.
Generate synthetic surveys above the same synthetic model and compute the
gravitational effect of the model on each survey.
Use each source distribution to predict values based on the ones obtained on
each survey onto the same grid points of the \emph{target grid}.
Score each interpolation by comparing the predicted values with the
\emph{target grid}.
Finally compare the accuracy of the predictions made by each source
distribution.

\subsection{Synthetic Model}

Describe forward model made out of prisms.
Show prisms model?

\subsection{Synthetic Surveys}

Create synthetic surveys to produce observation points.
In order to compare each source layout under very different set of observation
points we create a ground and an airborne synthetic surveys.
We selected a portion of the Southern Africa Gravity Data obtained by Dr. R.J.
Kleywegt et al.\ and made available through the NOAA website
(\url{https://www.ngdc.noaa.gov/mgg/gravity/gravity.html}) for creating
a synthetic ground survey.
We selected a portion of the Great Britain Aeromagnetic Survey acquired by
Hunting Geology and Geophysics Ltd and Canadian Aeroservices Ltd between 1955
and 1965 and made available by the
\href{
    https://www.bgs.ac.uk/products/geophysics/aeromagneticRegional.html
    }{
    British Geological Survey (BGS)
}
, for creating a synthetic airborne survey.

The gravitation effect of the synthetic model has been computed on both surveys
and added Gaussian noise (standard deviation of 1mGal) to simulate acquisition
errors.

\subsection{Comparisons}

%Describe the target grid. Show target grid figure.
%Describe the parameters used for each source distribution, how do we score
%each interpolation.
%Describe the Harmonica implementation of the EQL.
%Show results.

Compute the gravitational effect of the synthetic model on a regular grid of
points located at the same height.
This will be called our \emph{target grid}.
We will use it to compute the accuracy of the predictions made by each source
distribution.

Take a source distribution, fit the source coefficients and then use them to
predict data on the grid points where the \emph{target grid} is defined.
Define prediction error as the difference between the predicted values and the
\emph{target grid}.

Each source distribution has parameters that need to be specified before
fitting its coefficients.
For example, the \emph{grid of sources} needs the depth at which the sources
will be located.
The damping parameter used on the fitting process must also be specified
beforehand.
These parameters play an important role on the accuracy of the prediction.
We compute several predictions made by the same source distribution, but with
different values for its parameters.
Then we score each interpolation by computing the R$^2$ between each prediction
and the target grid.
We keep the best prediction, i.e.\ the one that has the highest R$^2$ score.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgements}

We are indebted to the developers and maintainers of the open-source software
without which this work would not have been possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
